{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Shapes:\n",
      "img_shape:\t (28, 28, 1)\n",
      "x_reader:\t (60000, 784)\n",
      "y_reader:\t (60000, 10)\n",
      "x_writer:\t (60000, 28, 28, 1)\n",
      "y_writer:\t (60000, 1)\n"
     ]
    }
   ],
   "source": [
    "## Imports ##\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import layers, optimizers, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from IPython.display import display, SVG\n",
    "\n",
    "## Constants\n",
    "num_classes = 10\n",
    "img_rows, img_cols, img_chan = 28, 28, 1\n",
    "img_shape = (img_rows, img_cols, img_chan)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "verbose = 0\n",
    "\n",
    "## Data ##\n",
    "(x_reader, y_reader), (_, _) = mnist.load_data()\n",
    "x_writer, y_writer = x_reader, y_reader\n",
    "\n",
    "x_reader = x_reader.reshape(x_reader.shape[0], img_rows * img_cols)\n",
    "x_reader = x_reader.astype(np.float32) / 255.0\n",
    "\n",
    "x_writer = (x_writer.astype(np.float32) - 127.5) / 127.5 # Rescale -1 to 1\n",
    "x_writer = np.expand_dims(x_writer, axis=3)\n",
    "\n",
    "y_reader = to_categorical(y_reader, num_classes)\n",
    "y_writer = y_writer.reshape(-1, 1)\n",
    "\n",
    "history = [[],[],[],[]]\n",
    "\n",
    "## Hyperparameters ##\n",
    "# Reading\n",
    "reader_epochs = 40\n",
    "reader_validation_split = 0.2\n",
    "reader_batch_size = 512\n",
    "reader_kernel_initializer = RandomUniform(minval=0.0000001, \n",
    "                                          maxval=0.0001, \n",
    "                                          seed=None)\n",
    "reader_optimizer = optimizers.Adam()\n",
    "reader_loss = categorical_crossentropy\n",
    "\n",
    "# Writing\n",
    "writer_epochs = 22000\n",
    "writer_latent_dim = 100\n",
    "writer_batch_size = 32\n",
    "writer_half_batch = int(writer_batch_size / 2)\n",
    "\n",
    "writer_optimizer = optimizers.Adam(lr=0.0002,      # lr default:         0.001\n",
    "                                    beta_1=0.5,     # beta_1 default:     0.9\n",
    "                                    beta_2=0.999, \n",
    "                                    epsilon=None, \n",
    "                                    decay=0.0, \n",
    "                                    amsgrad=False)\n",
    "writer_loss = ['binary_crossentropy']\n",
    "\n",
    "grader_optimizer = writer_optimizer\n",
    "grader_loss = ['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "\n",
    "school_optimizer = grader_optimizer\n",
    "school_loss = ['binary_crossentropy','sparse_categorical_crossentropy']\n",
    "\n",
    "print(\"Matrix Shapes:\")\n",
    "print(\"img_shape:\\t\", img_shape)\n",
    "print('x_reader:\\t', x_reader.shape)\n",
    "print('y_reader:\\t', y_reader.shape)\n",
    "print('x_writer:\\t', x_writer.shape)\n",
    "print('y_writer:\\t', y_writer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Sequential()\n",
    "reader.add(layers.Dense(320, activation='relu', \n",
    "                        kernel_initializer=reader_kernel_initializer, \n",
    "                        input_shape=[x_reader.shape[1]]))\n",
    "reader.add(layers.Dropout(0.66))\n",
    "reader.add(layers.Dense(num_classes, activation='softmax'))\n",
    "reader.compile(loss=reader_loss, \n",
    "               optimizer=reader_optimizer, \n",
    "               metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Layers: 2 layers: noise, class_labels\n",
    "noise = layers.Input(shape=(writer_latent_dim,))\n",
    "label = layers.Input(shape=(1,), dtype='int32')\n",
    "\n",
    "# Create an embedding\n",
    "label_embedding = layers.Embedding(num_classes, writer_latent_dim)(label)\n",
    "label_embedding = layers.Flatten()(label_embedding)\n",
    "\n",
    "writer_input = layers.multiply([noise, label_embedding])\n",
    "\n",
    "# Generator hidden layers\n",
    "writer_hidden = layers.Dense(128 * 7 * 7, \n",
    "                             activation='relu', \n",
    "                             input_dim=writer_latent_dim)(writer_input)\n",
    "writer_hidden = layers.Reshape((7, 7, 128))(writer_hidden)\n",
    "writer_hidden = layers.BatchNormalization(momentum=0.8)(writer_hidden)\n",
    "writer_hidden = layers.UpSampling2D()(writer_hidden)\n",
    "writer_hidden = layers.Conv2D(128, \n",
    "                              activation='relu',\n",
    "                              kernel_size=3, \n",
    "                              padding='same')(writer_hidden)\n",
    "writer_hidden = layers.BatchNormalization(momentum=0.8)(writer_hidden)\n",
    "writer_hidden = layers.UpSampling2D()(writer_hidden)\n",
    "writer_hidden = layers.Conv2D(64, \n",
    "                              activation='relu', \n",
    "                              kernel_size=3, \n",
    "                              padding='same')(writer_hidden)\n",
    "writer_hidden = layers.BatchNormalization(momentum=0.8)(writer_hidden)\n",
    "writer_image = layers.Conv2D(img_chan, \n",
    "                              activation='tanh',\n",
    "                              kernel_size=3, \n",
    "                              padding='same')(writer_hidden)\n",
    "\n",
    "# Finalize the model\n",
    "writer = Model([noise, label], writer_image)\n",
    "writer.compile(loss=writer_loss, \n",
    "               optimizer=writer_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_image = layers.Input(shape=img_shape)\n",
    "\n",
    "grader_hidden = layers.Conv2D(16, \n",
    "                              kernel_size=3, \n",
    "                              strides=2,  \n",
    "                              padding='same', \n",
    "                              input_shape=img_shape)(grader_image)\n",
    "grader_hidden = layers.LeakyReLU(alpha=0.2)(grader_hidden)\n",
    "grader_hidden = layers.Dropout(0.25)(grader_hidden)\n",
    "grader_hidden = layers.Conv2D(32, \n",
    "                              kernel_size=3, \n",
    "                              strides=2, \n",
    "                              padding='same')(grader_hidden)\n",
    "grader_hidden = layers.ZeroPadding2D(padding=((0,1),(0,1)))(grader_hidden)\n",
    "grader_hidden = layers.LeakyReLU(alpha=0.2)(grader_hidden)\n",
    "grader_hidden = layers.Dropout(0.25)(grader_hidden)\n",
    "grader_hidden = layers.BatchNormalization(momentum=0.8)(grader_hidden)\n",
    "grader_hidden = layers.Conv2D(64, \n",
    "                              kernel_size=3, \n",
    "                              strides=2, \n",
    "                              padding='same')(grader_hidden)\n",
    "grader_hidden = layers.LeakyReLU(alpha=0.2)(grader_hidden)\n",
    "grader_hidden = layers.Dropout(0.25)(grader_hidden)\n",
    "grader_hidden = layers.BatchNormalization(momentum=0.8)(grader_hidden)\n",
    "grader_hidden = layers.Conv2D(128, \n",
    "                              kernel_size=3, \n",
    "                              strides=1, \n",
    "                              padding='same')(grader_hidden)\n",
    "grader_hidden = layers.LeakyReLU(alpha=0.2)(grader_hidden)\n",
    "grader_hidden = layers.Dropout(0.25)(grader_hidden)\n",
    "grader_hidden = layers.Flatten()(grader_hidden)\n",
    "\n",
    "grader_valid = layers.Dense(1, \n",
    "                            activation='sigmoid')(grader_hidden)\n",
    "grader_label = layers.Dense(num_classes + 1, \n",
    "                            activation='softmax')(grader_hidden)\n",
    "\n",
    "grader = Model(grader_image, [grader_valid, grader_label])\n",
    "grader.compile(loss=grader_loss, \n",
    "               optimizer=grader_optimizer, \n",
    "               metrics=metrics)\n",
    "\n",
    "# Don't update discriminator during generator training (moving target problem)\n",
    "grader.trainable = False\n",
    "\n",
    "# Don't recompile the discriminator so may still be trained independently...\n",
    "grader_valid, grader_label = grader(writer_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### School house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined model takes generator inputs and has discriminator outputs...\n",
    "school = Model([noise, label], [grader_valid, grader_label])\n",
    "school.compile(loss=school_loss, \n",
    "               optimizer=school_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVG(model_to_dot(reader).create(prog='dot', format='svg'))\n",
    "# SVG(model_to_dot(writer).create(prog='dot', format='svg'))\n",
    "# SVG(model_to_dot(grader).create(prog='dot', format='svg'))\n",
    "# SVG(model_to_dot(school).create(prog='dot', format='svg'))\n",
    "\n",
    "# reader.summary()\n",
    "# writer.summary()\n",
    "# grader.summary()\n",
    "# school.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_hist = reader.fit(x_reader, y_reader, \n",
    "                         epochs=reader_epochs, \n",
    "                         batch_size=reader_batch_size, \n",
    "                         verbose=verbose, \n",
    "                         validation_split=reader_validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this needs to have the reader reporting the img class labels\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    idx = np.random.randint(0, x_train.shape[0], half_batch_size)\n",
    "    real_images = x_train[idx]\n",
    "\n",
    "    # Generated images\n",
    "    input_noise = np.random.normal(0, 1, (half_batch_size, latent_dim))\n",
    "    input_labels = np.random.randint(0, 10, half_batch_size).reshape(-1, 1)\n",
    "    generated_images = generator.predict([input_noise, input_labels])\n",
    "    \n",
    "    valid = np.ones((half_batch_size, 1)) # 1.0 real\n",
    "    fake = np.zeros((half_batch_size, 1)) # 0.0 fake\n",
    "    \n",
    "    # Use the labeled classes for the real images...\n",
    "    image_labels = y_train[idx]\n",
    "    # Assign the fake images to the \"extra class\" or \"fake class\"\n",
    "    # on the one-hot encoding for all fake images (they are\n",
    "    # not any of the digits 0-9 since they are -fakes- so\n",
    "    # we don't used the requested generator labels...)\n",
    "    fake_labels = 10 * np.ones(half_batch_size).reshape(-1, 1)\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch(real_images, [valid, image_labels])\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_images, [fake, fake_labels])\n",
    "\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # Training the generator...\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    # Note that we are using the combined model to backprop the loss to the generator... \n",
    "    valid = np.ones((batch_size, 1))\n",
    "    # Give them some labels so the generator can learn which digit\n",
    "    # it is trying to fake.\n",
    "    sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "    \n",
    "    # Train the generator\n",
    "    g_loss = combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])\n",
    "\n",
    "    history[0] += [d_loss[0]]\n",
    "    history[1] += [d_loss[3]]\n",
    "    history[2] += [d_loss[4]]\n",
    "    history[3] += [g_loss[0]]\n",
    "    \n",
    "    # Print progress indicator\n",
    "    print(\"\\r%d [Discriminator Loss: %f, Real/Fake-Acc.: %.2f%%, Classification-Acc: %.2f%%] [Generator Loss: %f]\" % \n",
    "          (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0]), end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
